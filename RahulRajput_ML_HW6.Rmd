---
title: "RahulRajput_ML_HW6"
author: "Rahul Rajput"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

QUESTION 1

1.1 Principle Component Analysis takes advantage of the inherent correlations present in data and reorients the data into a new coordinate system such that the same amount of variation can be represented on fewer axes, or in fewer dimensions. It does this by finding the single axis which explains the largest variation in the sample and then adds new subsequent orthogonal axes which explain the maximum remaining variation in the sample after removing the first dimension, continuing the process till all the original variation has been accounted for.

1.2 1. A major limitation of PCA is the lack of interpretability of the output. Since the coordinate system has been rotated,     the variables (dimensions) now have different meanings than the original ones.
    2. It is sensitive to outliers.
    3. PCA is best suited for linear relationships between features in the data.
    4. PCA would not work for cases where there is little correlation between features in the data.
    
    
QUESTION 2

2.1 Classification Trees: Outcomes are probabilities
    Starting with the given covariates at the root node, the decision tree algorithm finds all possible combinations of a        single split from all the covariates such that the two resulting child sets are as homogenous in their respective            response y as possible, or in other terms, we try to make each child node as 'pure' as possible. Basicall ensuring that      we maximise responses of a class within each node. A perfectly impure node would be one where there are equal number of      class responses. Mathematically there are several measures to ensure the optimal split, for example we can  minimise the     deviance between the two childs given by - SS(log(Pi)) or minimising the Gini measure given by (1 - Sum(Pi|j)^2), which      would be 0 for a node where all the responses belong to one class.

2.2 Regression Trees: Outcomes are continuous variables
    Starting with the given covariates at the root node, the decision tree algorithm finds all possible combinations of a        single split from all the covariates such that the two resulting child sets are as homogenous in their respective            response y as possible, or in other terms, we ensure that the resulting deviance, given by  SS(Yk - Y_lefthat) + SS(Yk -     Y_righthat) for Regression Trees, is minimised. The algorithm then proceeds to find the 'optimal' split for each child       node and the tree grows.
    
    
QUESTION 3

The Decision Tree Algorithm is a Nonparametric, Greedy, and Recursive algorithm, which means it is possible for the Tree to overfit to the Training set. It will keep trying to find 'optimal' splits till it reaches the ideal case of each leaf having one class, unless otherwise not possible. To prevent this overfitting, the Trees must be pruned. This can be done by:
1. Pre-Pruning: This involves specifying limits for hyperparameters, for example the maximum depth, or the minimum number of samples per node.
2. Post-Pruning: Once the Tree has been allowed to run it's full extent possible, we start from the bottom and remove branches/nodes which contribute least to deviance reduction. Each prune step going from the bottom-up produces a different Tree, which can then be compared with each other though Out of Sample testing, basically Cross Validation.

QUESTION 4

Random Forest is a Bootstrapped Aggregation of Decision Trees from a Sample, or put simply, it is an average of multiple Decision Trees. The advantage of Random Forests lies in the ability of Decision Trees being able to model Non Linear relations very well. Decision Trees are able to account for self-interactions, non-constant variance, or differently scaled features better. Plus aggregating the measures from multiple Decision Trees allows it to cancel out noise present within the sample. Only for a perfect linear model or sparse sample with very high dimensions would regressions perform better than a Random Forest.

QUESTION 5
```{r}
transaction = read.csv('Transaction.csv')
transaction$payment_default = as.factor(transaction$payment_default)
#transaction

set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(transaction), replace=TRUE, prob=c(0.8,0.2))
train  <- transaction[sample, ]
test   <- transaction[!sample, ]

# Question 5.1 using CART
#install.packages("tree")

library(tree)

mytree = tree(payment_default ~., data=train)
print(mytree)
summary(mytree)
plot(mytree)
text(mytree, pretty=0)

sprintf("From the output of the tree we see that only three features were able to explain whether there would be a payment default or not, namely Pay0, Pay2, and Pay_Amt3. The other 23 variables are not significant in determining probability of default.")

predictions = predict(mytree, newdata=test, type = 'class')
actual = test$payment_default

library(caret)
confusionMatrix(actual, predictions)

sprintf("From the confusion matrix we can see that the overall accuracy for the model was about 80%%. The sensitivity for the model was approx 83%% implying that the model was able to predict defaulters correctly 83%% of the times. However the error rate of the model classifying people who did not default as defaulters was higher.")

# Running Cross Validation through Pruning:
cv_fit = cv.tree(mytree, FUN = prune.tree)
plot(cv_fit$size, cv_fit$dev, type = "b")

sprintf("From the output of the Cross Validation, we see that the original tree has the lowest error, therefore we proceed with that model.")


# Question 5.2 using Random Forest
library(randomForest)

# Training RandomForest on complete data (Random Forest function performs train-test splits)
myrandomforest_full = randomForest(payment_default ~., data=transaction, importance=TRUE)
print(myrandomforest_full)

# Training RandomForest on training data
myrandomforest = randomForest(payment_default ~., data=train, importance=TRUE)
print(myrandomforest)

# "OOB error is the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample." - From Wikipedia

sprintf("The prediction (OOB) error for both random forest models is around 18%%")

summary(myrandomforest)
plot(myrandomforest)

predictions_rf = predict(myrandomforest, newdata=test, type = 'class')
confusionMatrix(actual, predictions_rf)

sprintf("The output from the Random Forest model has a slightly higher accuracy than the basic CART tree ans has a slightly better sensitivity rate as well. But the differences may not be significant.")

plot(myrandomforest)
varImpPlot(myrandomforest)

sprintf("From the Variance Importance plot we see that Pay0 is the most important to correctly classify the default variable.")

sprintf("In the cases of both CART and Random Forests, the assumed threshold for cut off into default or not defualt is 0.5. In reality we would like to err on the side of safety and set the threshold for not default should be lower than 0.5.")
```

